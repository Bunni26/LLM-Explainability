# LLM Explainability System ğŸ”

A comprehensive system for explaining and interpreting Large Language Models (LLMs) using Groq-hosted LLaMA models.

## ğŸ¯ Features

### 1. Traditional Fine-tuning Paradigm
- Local Explanations (Feature Attribution, Attention Analysis)
- Global Explanations (Mechanistic Interpretability)
- Model Debugging & Analysis

### 2. Prompting Paradigm
- In-context Learning Analysis
- Chain-of-Thought Reasoning
- Representation Engineering
- Hallucination Detection

### 3. Explanation Evaluation
- Plausibility Assessment
- Faithfulness Metrics
- Counterfactual Testing

## ğŸš€ Getting Started

1. Clone the repository
```bash
git clone https://github.com/yourusername/llm-explainability.git
cd llm-explainability
```

2. Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. Set up environment variables
```bash
cp .env.example .env
# Add your Groq API key to .env file
```

5. Run the application
```bash
streamlit run app.py
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py                 # Streamlit main application
â”œâ”€â”€ explanations/          # Explanation implementations
â”œâ”€â”€ prompting/             # Prompt engineering tools
â”œâ”€â”€ evaluations/           # Explanation evaluation metrics
â”œâ”€â”€ models/               # Groq inference helpers
â”œâ”€â”€ ui/                   # Streamlit components
â”œâ”€â”€ data/                # Sample data & prompts
â””â”€â”€ tests/               # Unit tests
```

## ğŸ› ï¸ Development

Run tests:
```bash
pytest
```

## ğŸ“„ License

MIT License - see LICENSE file for details 